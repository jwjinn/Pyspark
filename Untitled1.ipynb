{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc23fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init(spark_home='/home/joo/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ddd23d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 11:38:15,496 WARN util.Utils: Your hostname, joo-IdeaPad-5-Pro-14ACN6 resolves to a loopback address: 127.0.1.1; using 192.168.0.14 instead (on interface wlp2s0)\n",
      "2023-01-31 11:38:15,496 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/joo/spark-3.2.2-bin-without-hadoop/jars/spark-unsafe_2.12-3.2.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/joo/.ivy2/cache\n",
      "The jars for the packages stored in: /home/joo/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1670e8a3-40ce-4bc8-9c5c-45bfacad3082;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/joo/spark-3.2.2-bin-without-hadoop/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 137ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1670e8a3-40ce-4bc8-9c5c-45bfacad3082\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n",
      "2023-01-31 11:38:15,982 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-01-31 11:38:16,206 INFO spark.SparkContext: Running Spark version 3.2.2\n",
      "2023-01-31 11:38:16,219 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-01-31 11:38:16,220 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-01-31 11:38:16,220 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-01-31 11:38:16,220 INFO spark.SparkContext: Submitted application: Test\n",
      "2023-01-31 11:38:16,236 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-01-31 11:38:16,246 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2023-01-31 11:38:16,247 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-01-31 11:38:16,279 INFO spark.SecurityManager: Changing view acls to: joo\n",
      "2023-01-31 11:38:16,279 INFO spark.SecurityManager: Changing modify acls to: joo\n",
      "2023-01-31 11:38:16,279 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-01-31 11:38:16,279 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-01-31 11:38:16,280 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(joo); groups with view permissions: Set(); users  with modify permissions: Set(joo); groups with modify permissions: Set()\n",
      "2023-01-31 11:38:16,809 INFO util.Utils: Successfully started service 'sparkDriver' on port 42141.\n",
      "2023-01-31 11:38:16,827 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-01-31 11:38:16,845 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-01-31 11:38:16,858 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-01-31 11:38:16,858 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-01-31 11:38:16,892 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-01-31 11:38:16,905 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-c4d300a2-72ac-40c9-a576-ade527acb4da\n",
      "2023-01-31 11:38:16,920 INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "2023-01-31 11:38:16,949 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-01-31 11:38:17,012 INFO util.log: Logging initialized @2326ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-01-31 11:38:17,055 INFO server.Server: jetty-9.4.44.v20210927; built: 2021-09-27T23:02:44.612Z; git: 8da83308eeca865e495e53ef315a249d63ba9332; jvm 11.0.17+8-post-Ubuntu-1ubuntu220.04\n",
      "2023-01-31 11:38:17,064 INFO server.Server: Started @2379ms\n",
      "2023-01-31 11:38:17,081 INFO server.AbstractConnector: Started ServerConnector@7d197f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-01-31 11:38:17,081 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2023-01-31 11:38:17,094 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1dfc4e60{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,096 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40b359b6{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,096 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a82a4f7{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,098 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33b4b008{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,098 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d9316b6{/stages,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,099 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31a6f23{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,099 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34ee00ab{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,100 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a88768{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,100 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ca83723{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,101 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@151c47d3{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,101 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c38ec82{/storage,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,102 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f48a54e{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,102 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@43b95284{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,102 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b74dfdd{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,103 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d1eb536{/environment,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,103 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32041bae{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,104 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1910e9e3{/executors,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,104 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19c7f060{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,104 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c1dc215{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,105 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33848a9d{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,110 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20c4a315{/static,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,111 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6011cd7f{/,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,111 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28a511f3{/api,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,112 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7decdd52{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,112 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ed77928{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:17,113 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.14:4040\n",
      "2023-01-31 11:38:17,124 INFO spark.SparkContext: Added JAR file:/usr/share/java/mysql-connector-j-8.0.31.jar at spark://192.168.0.14:42141/jars/mysql-connector-j-8.0.31.jar with timestamp 1675132696202\n",
      "2023-01-31 11:38:17,288 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-01-31 11:38:17,425 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 11:38:17,678 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-31 11:38:17,678 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-31 11:38:17,687 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n",
      "2023-01-31 11:38:17,687 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "2023-01-31 11:38:17,687 INFO yarn.Client: Setting up container launch context for our AM\n",
      "2023-01-31 11:38:17,689 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "2023-01-31 11:38:17,693 INFO yarn.Client: Preparing resources for our AM container\n",
      "2023-01-31 11:38:17,725 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2023-01-31 11:38:18,337 INFO yarn.Client: Uploading resource file:/tmp/spark-e465f8e0-872e-4446-a8e0-ca7fe009a948/__spark_libs__7328136851977000511.zip -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0007/__spark_libs__7328136851977000511.zip\n",
      "2023-01-31 11:38:18,758 INFO yarn.Client: Uploading resource file:/usr/share/java/mysql-connector-j-8.0.31.jar -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0007/mysql-connector-j-8.0.31.jar\n",
      "2023-01-31 11:38:18,779 INFO yarn.Client: Uploading resource file:/home/joo/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0007/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar\n",
      "2023-01-31 11:38:18,795 INFO yarn.Client: Uploading resource file:/home/joo/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0007/org.mongodb_mongodb-driver-sync-4.0.5.jar\n",
      "2023-01-31 11:38:18,809 INFO yarn.Client: Uploading resource file:/home/joo/.ivy2/jars/org.mongodb_bson-4.0.5.jar -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0007/org.mongodb_bson-4.0.5.jar\n",
      "2023-01-31 11:38:18,825 INFO yarn.Client: Uploading resource file:/home/joo/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0007/org.mongodb_mongodb-driver-core-4.0.5.jar\n",
      "2023-01-31 11:38:18,842 INFO yarn.Client: Uploading resource file:/home/joo/spark-3.2.2-bin-without-hadoop/python/lib/pyspark.zip -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0007/pyspark.zip\n",
      "2023-01-31 11:38:18,856 INFO yarn.Client: Uploading resource file:/home/joo/spark-3.2.2-bin-without-hadoop/python/lib/py4j-0.10.9.5-src.zip -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0007/py4j-0.10.9.5-src.zip\n",
      "2023-01-31 11:38:18,870 WARN yarn.Client: Same path resource file:///home/joo/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar added multiple times to distributed cache.\n",
      "2023-01-31 11:38:18,870 WARN yarn.Client: Same path resource file:///home/joo/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar added multiple times to distributed cache.\n",
      "2023-01-31 11:38:18,870 WARN yarn.Client: Same path resource file:///home/joo/.ivy2/jars/org.mongodb_bson-4.0.5.jar added multiple times to distributed cache.\n",
      "2023-01-31 11:38:18,870 WARN yarn.Client: Same path resource file:///home/joo/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar added multiple times to distributed cache.\n",
      "2023-01-31 11:38:18,950 INFO yarn.Client: Uploading resource file:/tmp/spark-e465f8e0-872e-4446-a8e0-ca7fe009a948/__spark_conf__13506130036860213132.zip -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0007/__spark_conf__.zip\n",
      "2023-01-31 11:38:19,380 INFO spark.SecurityManager: Changing view acls to: joo\n",
      "2023-01-31 11:38:19,380 INFO spark.SecurityManager: Changing modify acls to: joo\n",
      "2023-01-31 11:38:19,380 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-01-31 11:38:19,380 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-01-31 11:38:19,380 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(joo); groups with view permissions: Set(); users  with modify permissions: Set(joo); groups with modify permissions: Set()\n",
      "2023-01-31 11:38:19,392 INFO yarn.Client: Submitting application application_1675125923840_0007 to ResourceManager\n",
      "2023-01-31 11:38:19,419 INFO impl.YarnClientImpl: Submitted application application_1675125923840_0007\n",
      "2023-01-31 11:38:20,422 INFO yarn.Client: Application report for application_1675125923840_0007 (state: ACCEPTED)\n",
      "2023-01-31 11:38:20,424 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1675132699401\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://joo-IdeaPad-5-Pro-14ACN6:8088/proxy/application_1675125923840_0007/\n",
      "\t user: joo\n",
      "2023-01-31 11:38:21,425 INFO yarn.Client: Application report for application_1675125923840_0007 (state: ACCEPTED)\n",
      "2023-01-31 11:38:22,427 INFO yarn.Client: Application report for application_1675125923840_0007 (state: ACCEPTED)\n",
      "2023-01-31 11:38:23,428 INFO yarn.Client: Application report for application_1675125923840_0007 (state: ACCEPTED)\n",
      "2023-01-31 11:38:24,299 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> joo-IdeaPad-5-Pro-14ACN6, PROXY_URI_BASES -> http://joo-IdeaPad-5-Pro-14ACN6:8088/proxy/application_1675125923840_0007), /proxy/application_1675125923840_0007\n",
      "2023-01-31 11:38:24,430 INFO yarn.Client: Application report for application_1675125923840_0007 (state: RUNNING)\n",
      "2023-01-31 11:38:24,430 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 192.168.0.14\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1675132699401\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://joo-IdeaPad-5-Pro-14ACN6:8088/proxy/application_1675125923840_0007/\n",
      "\t user: joo\n",
      "2023-01-31 11:38:24,431 INFO cluster.YarnClientSchedulerBackend: Application application_1675125923840_0007 has started running.\n",
      "2023-01-31 11:38:24,437 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35817.\n",
      "2023-01-31 11:38:24,437 INFO netty.NettyBlockTransferService: Server created on 192.168.0.14:35817\n",
      "2023-01-31 11:38:24,438 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-01-31 11:38:24,444 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.14, 35817, None)\n",
      "2023-01-31 11:38:24,448 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.14:35817 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.14, 35817, None)\n",
      "2023-01-31 11:38:24,449 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.14, 35817, None)\n",
      "2023-01-31 11:38:24,450 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.14, 35817, None)\n",
      "2023-01-31 11:38:24,561 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-01-31 11:38:24,563 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@467ebfbc{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:38:24,793 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "2023-01-31 11:38:30,238 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.0.14:57960) with ID 1,  ResourceProfileId 0\n",
      "2023-01-31 11:38:30,273 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.0.14:57958) with ID 2,  ResourceProfileId 0\n",
      "2023-01-31 11:38:30,289 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "2023-01-31 11:38:30,337 INFO storage.BlockManagerMasterEndpoint: Registering block manager joo-IdeaPad-5-Pro-14ACN6:45841 with 434.4 MiB RAM, BlockManagerId(1, joo-IdeaPad-5-Pro-14ACN6, 45841, None)\n",
      "2023-01-31 11:38:30,375 INFO storage.BlockManagerMasterEndpoint: Registering block manager joo-IdeaPad-5-Pro-14ACN6:42221 with 434.4 MiB RAM, BlockManagerId(2, joo-IdeaPad-5-Pro-14ACN6, 42221, None)\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import random\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(appName = 'Test', master = 'yarn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a80f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7294bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 11:39:38,851 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "2023-01-31 11:39:38,853 INFO internal.SharedState: Warehouse path is 'file:/home/joo/Desktop/Develop/spark/spark연습/spark-warehouse'.\n",
      "2023-01-31 11:39:38,860 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-01-31 11:39:38,861 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4cad14e{/SQL,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:39:38,861 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-01-31 11:39:38,862 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@a504c8d{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:39:38,862 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-01-31 11:39:38,863 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71751f02{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:39:38,863 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-01-31 11:39:38,863 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57470040{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:39:38,864 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-01-31 11:39:38,864 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d3cd830{/static/sql,null,AVAILABLE,@Spark}\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('company').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc94cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRange = spark.range(1000).toDF('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa4ce9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 11:52:09,660 INFO codegen.CodeGenerator: Code generated in 83.783655 ms\n",
      "2023-01-31 11:52:09,704 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-01-31 11:52:09,713 INFO scheduler.DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-01-31 11:52:09,714 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-31 11:52:09,714 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-31 11:52:09,715 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-31 11:52:09,717 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-31 11:52:09,769 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.6 KiB, free 434.4 MiB)\n",
      "2023-01-31 11:52:09,786 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 434.4 MiB)\n",
      "2023-01-31 11:52:09,787 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.14:35817 (size: 5.0 KiB, free: 434.4 MiB)\n",
      "2023-01-31 11:52:09,789 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-31 11:52:09,799 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-01-31 11:52:09,799 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "2023-01-31 11:52:09,829 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (joo-IdeaPad-5-Pro-14ACN6, executor 2, partition 0, PROCESS_LOCAL, 4589 bytes) taskResourceAssignments Map()\n",
      "2023-01-31 11:52:09,984 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on joo-IdeaPad-5-Pro-14ACN6:42221 (size: 5.0 KiB, free: 434.4 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "2023-01-31 11:52:10,373 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 558 ms on joo-IdeaPad-5-Pro-14ACN6 (executor 2) (1/1)\n",
      "2023-01-31 11:52:10,374 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2023-01-31 11:52:10,377 INFO scheduler.DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 0.655 s\n",
      "2023-01-31 11:52:10,380 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-01-31 11:52:10,380 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\n",
      "2023-01-31 11:52:10,381 INFO scheduler.DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 0.676874 s\n",
      "\r",
      "                                                                                \r",
      "2023-01-31 11:52:10,402 INFO codegen.CodeGenerator: Code generated in 9.025539 ms\n"
     ]
    }
   ],
   "source": [
    "myRange.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
