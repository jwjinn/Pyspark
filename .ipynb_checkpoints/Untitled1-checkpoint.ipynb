{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f814a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init(spark_home='/home/joo/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "052c5334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 11:34:42,152 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "2023-01-31 11:34:42,153 INFO spark.SparkContext: Running Spark version 3.2.2\n",
      "2023-01-31 11:34:42,154 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-01-31 11:34:42,154 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-01-31 11:34:42,154 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-01-31 11:34:42,154 INFO spark.SparkContext: Submitted application: Test\n",
      "2023-01-31 11:34:42,155 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-01-31 11:34:42,157 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2023-01-31 11:34:42,158 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-01-31 11:34:42,159 INFO spark.SecurityManager: Changing view acls to: joo\n",
      "2023-01-31 11:34:42,159 INFO spark.SecurityManager: Changing modify acls to: joo\n",
      "2023-01-31 11:34:42,159 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-01-31 11:34:42,159 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-01-31 11:34:42,159 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(joo); groups with view permissions: Set(); users  with modify permissions: Set(joo); groups with modify permissions: Set()\n",
      "2023-01-31 11:34:42,176 INFO util.Utils: Successfully started service 'sparkDriver' on port 34835.\n",
      "2023-01-31 11:34:42,178 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-01-31 11:34:42,178 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-01-31 11:34:42,179 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-01-31 11:34:42,179 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-01-31 11:34:42,207 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-01-31 11:34:42,207 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-422907f9-5448-44d5-8a19-1421e5183b62\n",
      "2023-01-31 11:34:42,208 INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "2023-01-31 11:34:42,229 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-01-31 11:34:42,245 INFO server.Server: jetty-9.4.44.v20210927; built: 2021-09-27T23:02:44.612Z; git: 8da83308eeca865e495e53ef315a249d63ba9332; jvm 11.0.17+8-post-Ubuntu-1ubuntu220.04\n",
      "2023-01-31 11:34:42,246 INFO server.Server: Started @14715ms\n",
      "2023-01-31 11:34:42,248 INFO server.AbstractConnector: Started ServerConnector@4bbdac31{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-01-31 11:34:42,248 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2023-01-31 11:34:42,249 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36fad892{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,249 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21d34210{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,250 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e8c60d2{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,250 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f402c8c{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,250 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c950eb1{/stages,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,251 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@579ccd7f{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,251 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41fdf261{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,252 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ca5a9f7{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,252 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b12e603{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,253 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@cdcded0{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,253 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26b79123{/storage,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,253 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7850c41d{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,254 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@12d9120f{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,254 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7bc66027{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,255 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4468f32b{/environment,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,256 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@488e0d32{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,256 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@bdbf37c{/executors,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,257 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f9be1f7{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,257 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e24b4a4{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,258 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6deaffd6{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,258 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1be901da{/static,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,259 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d3bae6c{/,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,259 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d3eb2c{/api,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,259 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25be350f{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,260 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41abab5d{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:42,260 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.14:4040\n",
      "2023-01-31 11:34:42,269 INFO spark.SparkContext: Added JAR file:/usr/share/java/mysql-connector-j-8.0.31.jar at spark://192.168.0.14:34835/jars/mysql-connector-j-8.0.31.jar with timestamp 1675132482153\n",
      "2023-01-31 11:34:42,408 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-01-31 11:34:42,547 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2023-01-31 11:34:42,789 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-31 11:34:42,789 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-31 11:34:42,797 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n",
      "2023-01-31 11:34:42,798 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "2023-01-31 11:34:42,798 INFO yarn.Client: Setting up container launch context for our AM\n",
      "2023-01-31 11:34:42,799 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "2023-01-31 11:34:42,803 INFO yarn.Client: Preparing resources for our AM container\n",
      "2023-01-31 11:34:42,830 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 11:34:43,435 INFO yarn.Client: Uploading resource file:/tmp/spark-23c267e8-1946-45e5-9133-31f078689cc8/__spark_libs__4065092743877751405.zip -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0005/__spark_libs__4065092743877751405.zip\n",
      "2023-01-31 11:34:43,837 INFO yarn.Client: Uploading resource file:/usr/share/java/mysql-connector-j-8.0.31.jar -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0005/mysql-connector-j-8.0.31.jar\n",
      "2023-01-31 11:34:43,855 INFO yarn.Client: Uploading resource file:/home/joo/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0005/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar\n",
      "2023-01-31 11:34:43,871 INFO yarn.Client: Uploading resource file:/home/joo/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0005/org.mongodb_mongodb-driver-sync-4.0.5.jar\n",
      "2023-01-31 11:34:43,887 INFO yarn.Client: Uploading resource file:/home/joo/.ivy2/jars/org.mongodb_bson-4.0.5.jar -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0005/org.mongodb_bson-4.0.5.jar\n",
      "2023-01-31 11:34:43,902 INFO yarn.Client: Uploading resource file:/home/joo/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0005/org.mongodb_mongodb-driver-core-4.0.5.jar\n",
      "2023-01-31 11:34:43,917 INFO yarn.Client: Uploading resource file:/home/joo/spark-3.2.2-bin-without-hadoop/python/lib/pyspark.zip -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0005/pyspark.zip\n",
      "2023-01-31 11:34:43,932 INFO yarn.Client: Uploading resource file:/home/joo/spark-3.2.2-bin-without-hadoop/python/lib/py4j-0.10.9.5-src.zip -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0005/py4j-0.10.9.5-src.zip\n",
      "2023-01-31 11:34:43,948 WARN yarn.Client: Same path resource file:///home/joo/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar added multiple times to distributed cache.\n",
      "2023-01-31 11:34:43,949 WARN yarn.Client: Same path resource file:///home/joo/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar added multiple times to distributed cache.\n",
      "2023-01-31 11:34:43,949 WARN yarn.Client: Same path resource file:///home/joo/.ivy2/jars/org.mongodb_bson-4.0.5.jar added multiple times to distributed cache.\n",
      "2023-01-31 11:34:43,949 WARN yarn.Client: Same path resource file:///home/joo/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar added multiple times to distributed cache.\n",
      "2023-01-31 11:34:44,032 INFO yarn.Client: Uploading resource file:/tmp/spark-23c267e8-1946-45e5-9133-31f078689cc8/__spark_conf__7274488193297803811.zip -> hdfs://localhost:9000/user/joo/.sparkStaging/application_1675125923840_0005/__spark_conf__.zip\n",
      "2023-01-31 11:34:44,065 INFO spark.SecurityManager: Changing view acls to: joo\n",
      "2023-01-31 11:34:44,065 INFO spark.SecurityManager: Changing modify acls to: joo\n",
      "2023-01-31 11:34:44,065 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-01-31 11:34:44,065 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-01-31 11:34:44,065 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(joo); groups with view permissions: Set(); users  with modify permissions: Set(joo); groups with modify permissions: Set()\n",
      "2023-01-31 11:34:44,080 INFO yarn.Client: Submitting application application_1675125923840_0005 to ResourceManager\n",
      "2023-01-31 11:34:44,108 INFO impl.YarnClientImpl: Submitted application application_1675125923840_0005\n",
      "2023-01-31 11:34:45,112 INFO yarn.Client: Application report for application_1675125923840_0005 (state: ACCEPTED)\n",
      "2023-01-31 11:34:45,114 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1675132484089\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://joo-IdeaPad-5-Pro-14ACN6:8088/proxy/application_1675125923840_0005/\n",
      "\t user: joo\n",
      "2023-01-31 11:34:46,115 INFO yarn.Client: Application report for application_1675125923840_0005 (state: ACCEPTED)\n",
      "2023-01-31 11:34:46,828 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> joo-IdeaPad-5-Pro-14ACN6, PROXY_URI_BASES -> http://joo-IdeaPad-5-Pro-14ACN6:8088/proxy/application_1675125923840_0005), /proxy/application_1675125923840_0005\n",
      "2023-01-31 11:34:47,116 INFO yarn.Client: Application report for application_1675125923840_0005 (state: RUNNING)\n",
      "2023-01-31 11:34:47,117 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 192.168.0.14\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1675132484089\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://joo-IdeaPad-5-Pro-14ACN6:8088/proxy/application_1675125923840_0005/\n",
      "\t user: joo\n",
      "2023-01-31 11:34:47,118 INFO cluster.YarnClientSchedulerBackend: Application application_1675125923840_0005 has started running.\n",
      "2023-01-31 11:34:47,124 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43587.\n",
      "2023-01-31 11:34:47,124 INFO netty.NettyBlockTransferService: Server created on 192.168.0.14:43587\n",
      "2023-01-31 11:34:47,125 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-01-31 11:34:47,125 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.14, 43587, None)\n",
      "2023-01-31 11:34:47,126 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.14:43587 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.14, 43587, None)\n",
      "2023-01-31 11:34:47,128 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.14, 43587, None)\n",
      "2023-01-31 11:34:47,129 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.14, 43587, None)\n",
      "2023-01-31 11:34:47,214 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-01-31 11:34:47,215 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b74a1ad{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-01-31 11:34:47,307 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "2023-01-31 11:34:49,415 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.0.14:47624) with ID 1,  ResourceProfileId 0\n",
      "2023-01-31 11:34:49,517 INFO storage.BlockManagerMasterEndpoint: Registering block manager joo-IdeaPad-5-Pro-14ACN6:43233 with 434.4 MiB RAM, BlockManagerId(1, joo-IdeaPad-5-Pro-14ACN6, 43233, None)\n",
      "2023-01-31 11:34:50,615 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.0.14:47650) with ID 2,  ResourceProfileId 0\n",
      "2023-01-31 11:34:50,632 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "2023-01-31 11:34:50,719 INFO storage.BlockManagerMasterEndpoint: Registering block manager joo-IdeaPad-5-Pro-14ACN6:33415 with 434.4 MiB RAM, BlockManagerId(2, joo-IdeaPad-5-Pro-14ACN6, 33415, None)\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import random\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(appName = 'Test', master = 'yarn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "021cc24b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspark\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spark'"
     ]
    }
   ],
   "source": [
    "import spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
